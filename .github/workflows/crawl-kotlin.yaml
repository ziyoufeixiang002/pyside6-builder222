name: Crawl Kotlin Documentation - Optimized

on:
  workflow_dispatch:

jobs:
  crawl-kotlin-docs:
    runs-on: ubuntu-latest
    timeout-minutes: 330
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Install tools
      run: |
        sudo apt-get update
        sudo apt-get install -y wget python3 python3-pip
        pip3 install requests beautifulsoup4 lxml urllib3
      
    - name: Create directories
      run: |
        mkdir -p kotlin-docs
        mkdir -p logs
        
    - name: Generate and validate URL list
      run: |
        # 生成URL列表并验证有效性
        cat > generate_and_validate_urls.py << 'EOF'
        import requests
        from bs4 import BeautifulSoup
        import urllib.parse
        import time
        import concurrent.futures
        
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; Documentation-Crawler/1.0)'
        })
        
        def check_url(url):
            """检查URL是否可访问"""
            try:
                response = session.head(url, timeout=10, allow_redirects=True)
                if response.status_code == 200:
                    return url, True
                else:
                    print(f"跳过 {url} - 状态码: {response.status_code}")
                    return url, False
            except Exception as e:
                print(f"跳过 {url} - 错误: {e}")
                return url, False
        
        def get_all_links(base_url, max_level=2):
            """获取所有链接，限制深度以提高成功率"""
            visited = set()
            to_visit = [(base_url, 0)]
            all_links = set()
            
            while to_visit:
                url, level = to_visit.pop(0)
                
                if url in visited or level > max_level:
                    continue
                    
                visited.add(url)
                print(f"扫描: {url} (层级 {level})")
                
                try:
                    response = session.get(url, timeout=15)
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    for link in soup.find_all('a', href=True):
                        href = link['href']
                        full_url = urllib.parse.urljoin(url, href)
                        
                        # 只保留docs和api开头的链接，且过滤媒体文件
                        if ('kotlinlang.org/docs' in full_url or 'kotlinlang.org/api' in full_url) and \
                           not any(ext in full_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp', '.mp3', '.wav', '.ogg', '.mp4', '.avi', '.mov', '.flv', '.webm', '.pdf', '.js', '.css', '.zip']):
                            if full_url not in visited and level < max_level:
                                to_visit.append((full_url, level + 1))
                                all_links.add(full_url)
                    
                    time.sleep(0.5)  # 礼貌等待
                    
                except Exception as e:
                    print(f"错误处理 {url}: {e}")
                    continue
                    
            return all_links
        
        print("开始收集docs链接...")
        docs_links = get_all_links('https://kotlinlang.org/docs/')
        print(f"找到 {len(docs_links)} 个docs链接")
        
        print("开始收集api链接...")
        api_links = get_all_links('https://kotlinlang.org/api/')
        print(f"找到 {len(api_links)} 个api链接")
        
        all_links = list(docs_links | api_links)
        print(f"总共找到 {len(all_links)} 个链接")
        
        # 验证链接可访问性
        print("验证链接可访问性...")
        valid_links = []
        
        # 使用线程池验证，但限制并发数
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            future_to_url = {executor.submit(check_url, url): url for url in all_links}
            for future in concurrent.futures.as_completed(future_to_url):
                url, valid = future.result()
                if valid:
                    valid_links.append(url)
        
        print(f"有效链接: {len(valid_links)}/{len(all_links)}")
        
        with open('valid_urls.txt', 'w') as f:
            for url in valid_links:
                f.write(url + '\n')
                
        with open('all_urls.txt', 'w') as f:
            for url in all_links:
                f.write(url + '\n')
        EOF
        
        python3 generate_and_validate_urls.py
        echo "有效URL: $(wc -l < valid_urls.txt)/$(wc -l < all_urls.txt)"
      
    - name: Download with rate limiting
      run: |
        cd kotlin-docs
        
        # 使用更保守的wget设置，限制速率
        # 不使用并行下载，避免被限制
        timeout 4h wget \
          --input-file=../valid_urls.txt \
          --no-check-certificate \
          --timeout=45 \
          --tries=3 \
          --wait=2 \
          --random-wait \
          --html-extension \
          --convert-links \
          --restrict-file-names=unix \
          --user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36" \
          --progress=dot \
          --timestamping \
          --level=0 \
          --recursive \
          --page-requisites \
          --adjust-extension \
          --force-directories \
          --trust-server-names \
          --retry-connrefused \
          --continue \
          --output-file=../logs/wget.log || true
          
        echo "下载完成"
        
    - name: Analyze download results
      run: |
        echo "=== 下载分析 ==="
        total_urls=$(wc -l < valid_urls.txt)
        downloaded_files=$(find kotlin-docs -type f | wc -l)
        html_files=$(find kotlin-docs -name "*.html" | wc -l)
        
        echo "有效URL数: $total_urls"
        echo "下载文件数: $downloaded_files"
        echo "HTML文件数: $html_files"
        
        if [ $total_urls -gt 0 ]; then
            success_rate=$((html_files * 100 / total_urls))
            echo "成功率: $success_rate%"
        fi
        
        # 检查wget日志中的错误
        if [ -f logs/wget.log ]; then
            echo "=== 常见错误 ==="
            grep -i "error\|failed\|404\|403\|500" logs/wget.log | head -10 || true
        fi
        
        # 生成详细报告
        cat > kotlin-docs/download-report.md << EOF
        # Kotlin 文档下载报告
        
        - 扫描到的总URL: $(wc -l < all_urls.txt)
        - 验证有效的URL: $(wc -l < valid_urls.txt)
        - 成功下载的HTML文件: $html_files
        - 下载成功率: $success_rate%
        - 下载时间: $(date)
        
        ## 文件结构
        \`\`\`
        $(find kotlin-docs -type d | sort | head -20)
        \`\`\`
        EOF
        
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: kotlin-docs-optimized
        path: |
          kotlin-docs/
          valid_urls.txt
          all_urls.txt
          logs/
        retention-days: 7
